{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# video rendering deps\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import IPython.display\n",
    "import imageio\n",
    "from IPython.display import Video\n",
    "\n",
    "import gym\n",
    "import gym_minigrid\n",
    "import hwm.gym_minigrid_2.fourroom_cstm # custom FourRoom envs\n",
    "from gym_minigrid.wrappers import ReseedWrapper\n",
    "from hwm.gym_minigrid_2.wrappers import RGBImgFullGridWrapper, ChannelFirstImgWrapper, \\\n",
    "    RGBImgResizeWrapper, ActionMaskingWrapper, FactoredStateRepWrapper, RenderWithoutHighlightWrapper\n",
    "\n",
    "from stable_baselines3 import PPO"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Dataset parameterization\n",
    "# Parallel environments\n",
    "\n",
    "# Set dataset filename\n",
    "dataset_filename = f\"datasets/MiniGrid-FourRooms-Size11-v0_SB3_PPOAgent_WithFactStates_40000\"\n",
    "padded_dataset_filename = f\"{dataset_filename}_Padded\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pre-processing dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# First, load the original dataset\n",
    "dataset = np.load(dataset_filename + '.npz', allow_pickle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Some dataset stats\n",
    "# checking the dataset, namely the consistency of the various fields saved\n",
    "print(\"obs. shape: \", dataset[\"observations\"][0].shape)\n",
    "print(\"rendered obs. shape: \", dataset[\"rendered_observations\"][0].shape)\n",
    "print(\"actions shape: \", dataset[\"actions\"][0].shape)\n",
    "print(\"rewards shape: \", dataset[\"rewards\"][0].shape)\n",
    "print(\"terminals shape: \", dataset[\"terminals\"][0].shape)\n",
    "print(\"factored_states shape: \", dataset[\"factored_states\"][0].shape)\n",
    "print(\"\")\n",
    "# Getting some stats from the dataset\n",
    "ep_lengths = [len(ep_terminal_list) for ep_terminal_list in dataset[\"terminals\"]]\n",
    "print(f\"Mean ep. length: {np.mean(ep_lengths)}\")\n",
    "print(f\"Median ep. length: {np.median(ep_lengths)}\")\n",
    "print(f\"Min. ep. length: {np.min(ep_lengths)}\")\n",
    "print(f\"Max. ep. length: {np.max(ep_lengths)}\")\n",
    "print(f\"Std. Dev. ep. length: {np.std(ep_lengths)}\")\n",
    "\n",
    "# print(train_dset[\"metadata\"])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "obs. shape:  (8, 3, 64, 64)\n",
      "rendered obs. shape:  (8, 352, 352, 3)\n",
      "actions shape:  (8, 3)\n",
      "rewards shape:  (8, 1)\n",
      "terminals shape:  (8, 1)\n",
      "factored_states shape:  (8, 36)\n",
      "\n",
      "Mean ep. length: 11.474756167527252\n",
      "Median ep. length: 11.0\n",
      "Min. ep. length: 3\n",
      "Max. ep. length: 20\n",
      "Std. Dev. ep. length: 3.2040600383600006\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "print(dataset[\"max_length\"])\n",
    "print(dataset[\"min_length\"])\n",
    "print(dataset[\"act_shape\"])\n",
    "print(dataset[\"rendered_observation_shape\"])\n",
    "print(dataset[\"n_episodes\"])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "20\n",
      "3\n",
      "3\n",
      "[352 352   3]\n",
      "3486\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Recover some metat data to use for the padding\n",
    "max_length, n_episodes, obs_shape, rendered_obs_shape, act_shape = \\\n",
    "    dataset[\"max_length\"], \\\n",
    "    dataset[\"n_episodes\"], \\\n",
    "    dataset[\"observation_shape\"], \\\n",
    "    dataset[\"rendered_observation_shape\"], \\\n",
    "    dataset[\"act_shape\"]\n",
    "\n",
    "# Equivalent to the hwm_init_size = 1 from previous experiments\n",
    "PAD_LEFT = 1\n",
    "PAD_RIGHT = 1\n",
    "\n",
    "PADDED_SEQ_LENGTH = PAD_LEFT + max_length + PAD_RIGHT\n",
    "\n",
    "real_episode_lengths = [len(dataset[\"terminals\"][ep_idx]) for ep_idx in range(n_episodes)]\n",
    "\n",
    "padded_dataset = {\n",
    "    \"observations\": np.zeros([n_episodes, PADDED_SEQ_LENGTH, *obs_shape], dtype=dataset[\"observations\"][0].dtype),\n",
    "    \"actions\": np.zeros([n_episodes, PADDED_SEQ_LENGTH, act_shape], dtype=np.uint8),\n",
    "    \"terminals\": np.zeros([n_episodes, PADDED_SEQ_LENGTH, 1], dtype=np.float32),\n",
    "    # NOTE: For now, don't really need rewards it seems\n",
    "    # \"rewards\": np.zeros([n_episodes, PADDED_SEQ_LENGTH, 1]),\n",
    "    # TODO: make this more memeory efficienct, as we don't really need padding here\n",
    "    \"rendered_observations\": dataset[\"rendered_observations\"],\n",
    "    \"factored_states\": dataset[\"factored_states\"],\n",
    "\n",
    "    ## NOTE: We use a custom masking scheme to make sure that the HWM losses are computed\n",
    "    ## only on the relevant parts of the padded trajectory. In other words, this does not take\n",
    "    ## into account the PAD_RIGHT and PAD_LEFT\n",
    "    \"depad_masks\": np.zeros([n_episodes, max_length, 1], dtype=np.float32),\n",
    "    ## NOTE: for each padded epeisode trajectory, holds the real start and end indicies, respectively\n",
    "    \"depad_slices\": np.zeros([n_episodes, 2], dtype=np.uint8),\n",
    "    \"unpadded_length\": np.zeros([n_episodes], dtype=np.uint8),\n",
    "    \n",
    "    # Default dataset\\s metadata\n",
    "    \"padded_length\": PADDED_SEQ_LENGTH,\n",
    "    \"max_length\": dataset[\"max_length\"],\n",
    "    \"min_length\": dataset[\"min_length\"],\n",
    "    \"act_shape\": dataset[\"act_shape\"],\n",
    "    \"factored_state_shape\": dataset[\"factored_state_shape\"],\n",
    "    \"observation_shape\": dataset[\"observation_shape\"],\n",
    "    \"rendered_observation_shape\": dataset[\"rendered_observation_shape\"],\n",
    "    \"n_episodes\": dataset[\"n_episodes\"]\n",
    "}\n",
    "fields_to_pad = [\"observations\", \"actions\", \"terminals\"]\n",
    "\n",
    "for ep_idx in tqdm(range(n_episodes)):\n",
    "    ep_real_length = real_episode_lengths[ep_idx]\n",
    "    ep_start_idx = PAD_RIGHT\n",
    "    ep_end_idx = PAD_RIGHT + ep_real_length\n",
    "\n",
    "    # ep_pad_right = PADDED_SEQ_LENGTH - (ep_real_length + PAD_RIGHT)\n",
    "    for field_name in fields_to_pad:\n",
    "        padded_dataset[field_name][ep_idx][ep_start_idx:ep_end_idx] = dataset[field_name][ep_idx]\n",
    "        padded_dataset[\"depad_masks\"][ep_idx][:(ep_end_idx-PAD_RIGHT)] = 1.\n",
    "        padded_dataset[\"depad_slices\"][ep_idx] = np.array([ep_start_idx, ep_end_idx], dtype=np.uint8)\n",
    "        padded_dataset[\"unpadded_length\"][ep_idx] = ep_real_length"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fe8ca6347bbc4df890a6726689842117"
      },
      "text/plain": [
       "  0%|          | 0/3486 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(dataset[\"actions\"][0].shape, padded_dataset[\"actions\"][0].shape)\n",
    "\n",
    "assert padded_dataset[\"observations\"].shape[:2] == padded_dataset[\"actions\"].shape[:2], \\\n",
    "    f'Padded observations and actions dimension do not match: {padded_dataset[\"observations\"].shape[:2]} vs {padded_dataset[\"actions\"].shape[:2]}'\n",
    "\n",
    "# dataset[\"actions\"][0], padded_dataset[\"actions\"][0]\n",
    "# start_idx, end_idx = padded_dataset[\"depad_slices\"][0][0], padded_dataset[\"depad_slices\"][0][1]\n",
    "assert np.array([(dataset[\"actions\"][i] == padded_dataset[\"actions\"][i][padded_dataset[\"depad_slices\"][i][0]:padded_dataset[\"depad_slices\"][i][1], :]).all() for i in range(n_episodes)]).all(), \\\n",
    "    \"Test failed while trying to depad the actions\"\n",
    "assert np.array([(dataset[\"observations\"][i] == padded_dataset[\"observations\"][i][padded_dataset[\"depad_slices\"][i][0]:padded_dataset[\"depad_slices\"][i][1], :]).all() for i in range(n_episodes)]).all(), \\\n",
    "    \"Test failed while trying to depad the observations\"\n",
    "assert np.array([(dataset[\"terminals\"][i] == padded_dataset[\"terminals\"][i][padded_dataset[\"depad_slices\"][i][0]:padded_dataset[\"depad_slices\"][i][1], :]).all() for i in range(n_episodes)]).all(), \\\n",
    "    \"Test failed while trying to depad the terminals\"\n",
    "# padded_dataset[\"actions\"][ep_idx].shape, padded_dataset[\"unpadded_length\"][ep_idx], len(dataset[\"terminals\"][ep_idx]), max_length, padded_dataset[\"depad_masks\"][ep_idx].squeeze(-1)\n",
    "assert np.array([np.where(padded_dataset[\"depad_masks\"][ep_idx].squeeze(-1) == 1)[0].max() == (len(dataset[\"terminals\"][ep_idx])-1) for ep_idx in range(n_episodes)]).all(), \\\n",
    "    \"Test failed while trying to check the depad_masks validity\"\n",
    "\n",
    "# Additional test on the depad_masks: the number of 1. in the mask should match the real length of the episode\n",
    "ep_idx = 0\n",
    "np.array([int(padded_dataset[\"depad_masks\"][ep_idx].sum()) == padded_dataset[\"unpadded_length\"][ep_idx] for ep_idx in range(n_episodes)]).all(), \\\n",
    "    \"Test failed while trying to check the depad_masks validity\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Data set saving. Make sure all the testsa above are passed\n",
    "np.savez_compressed(f\"{padded_dataset_filename}.npz\", **padded_dataset)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('hwm': conda)"
  },
  "interpreter": {
   "hash": "1894a78a4a0aae8caa0c39deebd3dba21f3d63f734dab84691c98aeb0f16a75c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# video rendering deps\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import IPython.display\n",
    "import imageio\n",
    "from IPython.display import Video\n",
    "\n",
    "import gym\n",
    "import gym_minigrid\n",
    "import hwm.gym_minigrid_2.fourroom_cstm # custom FourRoom envs\n",
    "import hwm.gym_minigrid_2.empty_cstm\n",
    "from gym_minigrid.wrappers import ReseedWrapper, ImgObsWrapper\n",
    "from hwm.gym_minigrid_2.wrappers import RGBImgFullGridWrapper, ChannelFirstImgWrapper, \\\n",
    "    RGBImgResizeWrapper, ActionMaskingWrapper, FactoredStateRepWrapper, RenderWithoutHighlightWrapper\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "# from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "import os\n",
    "from typing import Any, Callable, Dict, Optional, Type, Union\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv, VecEnv\n",
    "\n",
    "def make_vec_env(\n",
    "    env_id: Union[str, Type[gym.Env]],\n",
    "    n_envs: int = 1,\n",
    "    seed: Optional[int] = None,\n",
    "    start_index: int = 0,\n",
    "    monitor_dir: Optional[str] = None,\n",
    "    wrapper_class: Optional[Callable[[gym.Env], gym.Env]] = None,\n",
    "    env_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    vec_env_cls: Optional[Type[Union[DummyVecEnv, SubprocVecEnv]]] = None,\n",
    "    vec_env_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    monitor_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    wrapper_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    ) -> VecEnv:\n",
    "    \"\"\"\n",
    "    Create a wrapped, monitored ``VecEnv``.\n",
    "    By default it uses a ``DummyVecEnv`` which is usually faster\n",
    "    than a ``SubprocVecEnv``.\n",
    "\n",
    "    :param env_id: the environment ID or the environment class\n",
    "    :param n_envs: the number of environments you wish to have in parallel\n",
    "    :param seed: the initial seed for the random number generator\n",
    "    :param start_index: start rank index\n",
    "    :param monitor_dir: Path to a folder where the monitor files will be saved.\n",
    "        If None, no file will be written, however, the env will still be wrapped\n",
    "        in a Monitor wrapper to provide additional information about training.\n",
    "    :param wrapper_class: Additional wrapper to use on the environment.\n",
    "        This can also be a function with single argument that wraps the environment in many things.\n",
    "    :param env_kwargs: Optional keyword argument to pass to the env constructor\n",
    "    :param vec_env_cls: A custom ``VecEnv`` class constructor. Default: None.\n",
    "    :param vec_env_kwargs: Keyword arguments to pass to the ``VecEnv`` class constructor.\n",
    "    :param monitor_kwargs: Keyword arguments to pass to the ``Monitor`` class constructor.\n",
    "    :param wrapper_kwargs: Keyword arguments to pass to the ``Wrapper`` class constructor.\n",
    "    :return: The wrapped environment\n",
    "    \"\"\"\n",
    "    env_kwargs = {} if env_kwargs is None else env_kwargs\n",
    "    vec_env_kwargs = {} if vec_env_kwargs is None else vec_env_kwargs\n",
    "    monitor_kwargs = {} if monitor_kwargs is None else monitor_kwargs\n",
    "    wrapper_kwargs = {} if wrapper_kwargs is None else wrapper_kwargs\n",
    "\n",
    "    def make_env(rank):\n",
    "        def _init():\n",
    "            env = gym.make(env_id, **env_kwargs)\n",
    "            \n",
    "            if not wrapper_kwargs[\"no_reseed\"]:\n",
    "                env = ReseedWrapper(env, seeds=wrapper_kwargs[\"env_seeds\"])\n",
    "            else:\n",
    "                env.seed(seed + rank)\n",
    "                env.action_space.seed(seed + rank)\n",
    "\n",
    "            env = RGBImgFullGridWrapper(env)\n",
    "            env = RGBImgResizeWrapper(env, image_size=wrapper_kwargs[\"env_img_size\"])\n",
    "            env = ImgObsWrapper(env)\n",
    "            env = ChannelFirstImgWrapper(env)\n",
    "            \n",
    "            if len(wrapper_kwargs[\"env_masked_actions\"]):\n",
    "                env = ActionMaskingWrapper(env, invalid_actions_list=wrapper_kwargs[\"env_masked_actions\"])\n",
    "            \n",
    "            env = RenderWithoutHighlightWrapper(env)\n",
    "\n",
    "            # Wrap the env in a Monitor wrapper\n",
    "            # to have additional training information\n",
    "            monitor_path = os.path.join(monitor_dir, str(rank)) if monitor_dir is not None else None\n",
    "            # Create the monitor folder if needed\n",
    "            if monitor_path is not None:\n",
    "                os.makedirs(monitor_dir, exist_ok=True)\n",
    "            env = Monitor(env, filename=monitor_path, **monitor_kwargs)\n",
    "            # Optionally, wrap the environment with the provided wrapper\n",
    "            if wrapper_class is not None:\n",
    "                env = wrapper_class(env, **wrapper_kwargs)\n",
    "            return env\n",
    "\n",
    "        return _init\n",
    "\n",
    "    # No custom VecEnv is passed\n",
    "    if vec_env_cls is None:\n",
    "        # Default: use a DummyVecEnv\n",
    "        vec_env_cls = DummyVecEnv\n",
    "\n",
    "    return vec_env_cls([make_env(i + start_index) for i in range(n_envs)], **vec_env_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset parameterization\n",
    "# Parallel environments\n",
    "# ENV_NAME = \"MiniGrid-FourRooms-Size11-v0\"\n",
    "ENV_NAME = \"MiniGrid-Empty-11x11-v0\"\n",
    "\n",
    "wrapper_kwargs = {\n",
    "    \"no_reseed\": False,\n",
    "    \"env_seeds\": [222],\n",
    "    \"env_masked_actions\": [\"pickup\", \"drop\", \"toggle\", \"done\"],\n",
    "    \"env_img_size\": [84,84],\n",
    "}\n",
    "env = make_vec_env(ENV_NAME, n_envs=1, wrapper_kwargs=wrapper_kwargs)\n",
    "\n",
    "# IMPORTANT: Defines the size of the dataset\n",
    "TRAIN_DATASET_SIZE = int(25001)\n",
    "# Set dataset filename\n",
    "dataset_filename = f\"datasets/{ENV_NAME}_SB3_PPOAgent_WithHWMPaddedData_{TRAIN_DATASET_SIZE}\"\n",
    "\n",
    "# Agent SB3 model loading\n",
    "model = PPO.load(\"models/PPO_CNN_FullImgObs_MiniGrid-Empty-11x11-v0_Agent.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"/tmp/observer_video.mp4\" controls autoplay loop  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "dones = np.zeros(1)\n",
    "video_data = [env.render(mode=\"rgb_array\")]\n",
    "while dones.sum() < 1:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    video_data.append(env.render(mode=\"rgb_array\"))\n",
    "\n",
    "video_data = np.array(video_data, dtype=np.uint8)\n",
    "imageio.mimwrite('/tmp/observer_video.mp4', video_data, fps=4)\n",
    "IPython.display.display(Video(\"/tmp/observer_video.mp4\", html_attributes=\"controls autoplay loop\"))\n",
    "# video_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# agent_pov_obs_shape = agent_obs_data.shape[1:]\n",
    "observer_obs_shape = video_data.shape[1:]\n",
    "agent_pov_obs_shape = (64,64,3) # Note: PPO uses 84x84x3 though\n",
    "act_shape = env.action_space.n\n",
    "\n",
    "# NOTE: since we have defined max ep length, we could also do the padding otgether here \n",
    "MAX_EP_LENGTH = 20 # Defines the maximum length of an episode that is to be collected.\n",
    "observer_obs_shape, agent_pov_obs_shape, act_shape\n",
    "\n",
    "# Padding info\n",
    "PAD_RIGHT = PAD_LEFT = 1\n",
    "PADDED_SEQ_LENGTH = PAD_RIGHT + MAX_EP_LENGTH + PAD_LEFT\n",
    "\n",
    "# Helper for image scaling\n",
    "def scale_img(a, old_min=0., old_max=255., new_min=-1, new_max=1.):\n",
    "    assert old_min < old_max, f\"Invalid scaling: old_min {old_min} >= old_max: {old_max}\"\n",
    "    assert new_min < new_max, f\"Invalid scaling: new_min {new_min} >= new_max: {new_max}\"\n",
    "    return ((a - old_min) * (new_max - new_min)) / (old_max - old_min) + new_min\n",
    "\n",
    "def generate_dataset(dataset_filename, max_steps, mode=\"Train\"):\n",
    "    dataset = {\n",
    "        \"observations\": [],\n",
    "        \"actions\": [], # alreayd one hot\n",
    "        # \"next_observations\": np.zeros([DATASET_SIZE, *obs_shape]),\n",
    "        \"rewards\": [],\n",
    "        \"terminals\": [],\n",
    "        \"rendered_observations\": [], # Human observer observations, usually higher dimension\n",
    "\n",
    "        # Padded dataset\n",
    "        \"padded_observations\": [],\n",
    "        \"padded_actions\": [],\n",
    "        \"padded_terminals\": [],\n",
    "        \"depad_masks\": [],\n",
    "        \"depad_slices\": [],\n",
    "        \"unpadded_ep_lengths\": [],\n",
    "\n",
    "        # Metadata\n",
    "        \"padded_length\": PADDED_SEQ_LENGTH,\n",
    "        \"max_length\": MAX_EP_LENGTH, # Trajectory length, without the padding\n",
    "        \"act_shape\": act_shape,\n",
    "        \"rendered_observation_shape\": observer_obs_shape,\n",
    "    }\n",
    "\n",
    "    total_steps = 0\n",
    "    n_eps = 0\n",
    "\n",
    "    while total_steps < max_steps:\n",
    "        ep_obs, ep_rendered_obs, ep_actions, \\\n",
    "            ep_rewards, ep_terminals = \\\n",
    "            [], [], [], [], []\n",
    "        ep_length = 0\n",
    "\n",
    "        obs = env.reset()\n",
    "        dones = np.zeros(1)\n",
    "        \n",
    "        while dones.sum() < 1:\n",
    "            rendered_obs = env.render(mode=\"rgb_array\")\n",
    "\n",
    "            with th.no_grad():\n",
    "                action, _states = model.predict(obs)\n",
    "            # Do one environment step\n",
    "            next_obs, rewards, dones, info = env.step(action)\n",
    "            # Save the data of the current time step\n",
    "            onehot_action = np.eye(act_shape)[action[0]]\n",
    "\n",
    "            ep_actions.append(onehot_action)\n",
    "            ep_rewards.append(rewards)\n",
    "            ep_terminals.append(dones)\n",
    "            # TODO: add permute to C,H,W directy here. Reduce computation during the training.\n",
    "            ep_obs.append(scale_img(\n",
    "                cv2.resize(rendered_obs * 1., agent_pov_obs_shape[:2],\n",
    "                interpolation=cv2.INTER_AREA)))\n",
    "            ep_rendered_obs.append(rendered_obs)\n",
    "\n",
    "            obs = next_obs\n",
    "\n",
    "            ep_length += 1\n",
    "            # total_steps += 1\n",
    "\n",
    "        \n",
    "        # Episode done, append trajectory to the dataset in case it fits the various restrictions\n",
    "        # otherwise, sack it\n",
    "        # NOTE: using np.uint8 when possible decreased the required storage space\n",
    "        # at least in system memory\n",
    "        # Tranpose the observations from [T,H,W,C] to [T,C,H,W] to match Pytorchs requirement\n",
    "        # and save computation time during training.\n",
    "        # The observation is also scaled to the range of -1,1 here to reduce the repetitive\n",
    "        # computation during training\n",
    "        if ep_length <= MAX_EP_LENGTH:\n",
    "            dataset[\"observations\"].append(np.array(ep_obs, dtype=np.float32).transpose(0,3,1,2))\n",
    "            dataset[\"actions\"].append(np.array(ep_actions, dtype=np.uint8))\n",
    "            dataset[\"rewards\"].append(np.array(ep_rewards, dtype=np.float32))\n",
    "            dataset[\"terminals\"].append(np.array(ep_terminals, dtype=np.float32))\n",
    "            dataset[\"rendered_observations\"].append(np.array(ep_rendered_obs, dtype=np.uint8))\n",
    "\n",
    "            # adding the padded data\n",
    "            padded_observations = np.zeros([PADDED_SEQ_LENGTH, *ep_obs[-1].shape[::-1]], dtype=np.float32)\n",
    "            padded_observations[PAD_RIGHT:PAD_RIGHT+ep_length] = np.array(ep_obs, dtype=np.float32).transpose(0,3,1,2)\n",
    "\n",
    "            padded_actions = np.zeros([PADDED_SEQ_LENGTH, ep_actions[-1].shape[-1]], dtype=np.uint8)\n",
    "            padded_actions[PAD_RIGHT:PAD_RIGHT+ep_length] = np.array(ep_actions, dtype=np.uint8)\n",
    "\n",
    "            padded_terminals = np.zeros([PADDED_SEQ_LENGTH, 1], dtype=np.float32)\n",
    "            padded_terminals[PAD_RIGHT:PAD_RIGHT+ep_length] = np.array(ep_terminals, dtype=np.float32)\n",
    "\n",
    "            depad_mask = np.zeros([MAX_EP_LENGTH, 1], np.float32)\n",
    "            depad_mask[:ep_length] = 1.\n",
    "\n",
    "            depad_slice = np.array([PAD_RIGHT, PAD_RIGHT + ep_length], dtype=np.uint8)\n",
    "\n",
    "            # Checking that the padded data is valid\n",
    "            assert (padded_observations[depad_slice[0]:depad_slice[1]] == np.array(ep_obs, dtype=np.float32).transpose(0,3,1,2)).all(), \\\n",
    "                \"Test on the padded observation data failed\"\n",
    "            assert (padded_actions[depad_slice[0]:depad_slice[1]] == np.array(ep_actions, dtype=np.uint8)).all(), \\\n",
    "                \"Test on the padded actions data failed\"\n",
    "            assert (padded_terminals[depad_slice[0]:depad_slice[1]] == np.array(ep_terminals, dtype=np.float32)).all(), \\\n",
    "                \"Test on the padded terminals data failed\"\n",
    "            assert (int(np.sum(depad_mask)) == ep_length), \"Test on depad_mask failed.\"\n",
    "\n",
    "            dataset[\"padded_observations\"].append(padded_observations)\n",
    "            dataset[\"padded_actions\"].append(padded_actions)\n",
    "            dataset[\"padded_terminals\"].append(padded_terminals)\n",
    "            dataset[\"depad_masks\"].append(depad_mask)\n",
    "            dataset[\"depad_slices\"].append(depad_slice)\n",
    "            dataset[\"unpadded_ep_lengths\"].append(ep_length)\n",
    "\n",
    "            n_eps += 1\n",
    "            total_steps += ep_length\n",
    "\n",
    "        if n_eps % 100 == 0 :\n",
    "            print(f\"Sampled {n_eps} eps / Total steps: {total_steps}\")\n",
    "    # TODO: maybe add a metadata field into the dataset to stor ethe min and max lengths of trajectories\n",
    "    # in the dataset, and so on.\n",
    "\n",
    "    ep_lengths = [len(ep_terminal_list) for ep_terminal_list in dataset[\"terminals\"]]\n",
    "    dataset[\"max_ep_length\"] = int(np.max(ep_lengths))\n",
    "    dataset[\"min_ep_length\"] = int(np.min(ep_lengths))\n",
    "    dataset[\"act_shape\"] = act_shape\n",
    "    dataset[\"observation_shape\"] = ep_obs[-1].shape[::-1] # Breaks if the Height and width are different.\n",
    "    dataset[\"rendered_observation_shape\"] = ep_rendered_obs[-1].shape\n",
    "    dataset[\"n_episodes\"] = len(ep_lengths)\n",
    "\n",
    "    # Save the collected data dict\n",
    "    np.savez_compressed(f\"{dataset_filename}.npz\", **dataset)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 100 eps / Total steps: 1352\n",
      "Sampled 100 eps / Total steps: 1352\n",
      "Sampled 100 eps / Total steps: 1352\n",
      "Sampled 200 eps / Total steps: 2718\n",
      "Sampled 300 eps / Total steps: 4113\n",
      "Sampled 400 eps / Total steps: 5436\n",
      "Sampled 400 eps / Total steps: 5436\n"
     ]
    }
   ],
   "source": [
    "# Complete dset for HWM\n",
    "train_dset = generate_dataset(dataset_filename, TRAIN_DATASET_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the dataset, namely the consistency of the various fields saved\n",
    "print(\"obs. shape: \", train_dset[\"observations\"][0].shape)\n",
    "print(\"rendered obs. shape: \", train_dset[\"rendered_observations\"][0].shape)\n",
    "print(\"actions shape: \", train_dset[\"actions\"][0].shape)\n",
    "print(\"rewards shape: \", train_dset[\"rewards\"][0].shape)\n",
    "print(\"terminals shape: \", train_dset[\"terminals\"][0].shape)\n",
    "print(\"\")\n",
    "\n",
    "## padded data stats\n",
    "print(\"padded obs. shape: \", train_dset[\"padded_observations\"][0].shape)\n",
    "print(\"padded actions shape: \", train_dset[\"padded_actions\"][0].shape)\n",
    "print(\"padded terminals shape: \", train_dset[\"padded_terminals\"][0].shape)\n",
    "\n",
    "print(\"\")\n",
    "# Getting some stats from the dataset\n",
    "ep_lengths = [len(ep_terminal_list) for ep_terminal_list in train_dset[\"terminals\"]]\n",
    "print(f\"Mean ep. length: {np.mean(ep_lengths)}\")\n",
    "print(f\"Median ep. length: {np.median(ep_lengths)}\")\n",
    "print(f\"Min. ep. length: {np.min(ep_lengths)}\")\n",
    "print(f\"Max. ep. length: {np.max(ep_lengths)}\")\n",
    "print(f\"Std. Dev. ep. length: {np.std(ep_lengths)}\")\n",
    "\n",
    "# print(train_dset[\"metadata\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualizing the depad masks\n",
    "# ep_idx = 1\n",
    "# obs_list = train_dset[\"observations\"][ep_idx]\n",
    "# padded_obs_list = train_dset[\"padded_observations\"][ep_idx]\n",
    "# depad_mask = train_dset[\"depad_masks\"][ep_idx]\n",
    "# ep_real_length = train_dset[\"unpadded_ep_lengths\"][ep_idx]\n",
    "\n",
    "# obs_list[:, :, 0, 0], padded_obs_list[:, :, 0, 0], depad_mask, ep_real_length, len(padded_obs_list)\n",
    "# # depad_mask.reshape(-1,1,1,1).shape\n",
    "# # (padded_obs_list[1:-1] * depad_mask.reshape(-1,1,1,1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qualitative check of the data saved\n",
    "# Plotting stuff\n",
    "PLOT_SCALE = 1.\n",
    "AX_WIDTH, AX_HEIGHT = 2,2\n",
    "EP_TO_PLOT = len(ep_lengths) # N rows\n",
    "from hwm.gym_minigrid.metadata import ACTION_META_DICT\n",
    "from hwm.utils import draw_color_over_frame\n",
    "COOL_GREY = (0.55,0.57,0.67)\n",
    "COQUELICOT = (1.0, .22, .0)\n",
    "EGYPTION_BLUE = (0.06, 0.2, 0.65)\n",
    "# NOTE: if the env masks some actions, need to accout for it later\n",
    "# HWM Hyper parameters\n",
    "# hwm_init_size = 3\n",
    "\n",
    "for ep_idx, (ep_agent_obs_list, ep_action_list, ep_terminal_list) in \\\n",
    "    enumerate(zip(train_dset[\"observations\"], train_dset[\"actions\"], train_dset[\"terminals\"])):\n",
    "    ep_length = len(ep_agent_obs_list)\n",
    "\n",
    "    fig, axes = plt.subplots(1, ep_length, \n",
    "        gridspec_kw = {'wspace': 0.05, 'hspace': 0.1},\n",
    "        figsize=(AX_WIDTH * PLOT_SCALE * ep_length, 1 * AX_HEIGHT * PLOT_SCALE + 1.25)) # +1 is for to fit the title\n",
    "    axes[0].set_ylabel(\"Agent POV\", fontsize=14)\n",
    "    fig.patch.set_facecolor(\"white\")\n",
    "    \n",
    "    for t, (agent_obs, act, done) in enumerate(zip(ep_agent_obs_list, ep_action_list, ep_terminal_list)):\n",
    "        agent_obs = agent_obs.transpose(1,2,0) * 0.5 + 0.5\n",
    "\n",
    "        # if t < hwm_init_size or t >= ep_length + hwm_init_size:\n",
    "        #     agent_obs = draw_color_over_frame(agent_obs, COOL_GREY, normalized=False)\n",
    "        # if ep_gt_bnd_data_list[t][0] == 1.:\n",
    "        #     agent_obs = draw_color_over_frame(agent_obs, EGYPTION_BLUE, normalized=True, border_thicc=5)\n",
    "        if done == 1.0:\n",
    "            agent_obs = draw_color_over_frame(agent_obs, COQUELICOT, normalized=True, border_thicc=3)\n",
    "        axes[t].imshow(agent_obs)\n",
    "\n",
    "        act_idx = np.argmax(act)\n",
    "        axes[t].set_title(f\"t = {t}\\n{ACTION_META_DICT[act_idx]}\", fontsize=14)\n",
    "        # Remove the ticks and label\n",
    "        axes[t].tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n",
    "    \n",
    "    # fig.tight_layout()\n",
    "    fig.suptitle(f\"Ep. {ep_idx} | Ep. Length: {ep_length}\", fontsize=20)\n",
    "    fig.show()\n",
    "\n",
    "    # print(\"read_data upnadded: \", ep_padded_gt_bnd_data_list[1:-1, 0])\n",
    "    # print(\"copy_data upnadded: \", ep_padded_gt_bnd_data_list[1:-1, 1])\n",
    "    # plt.close(fig)\n",
    "    if ep_idx >= 10:\n",
    "        break # DEBUG for only one episode first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset[\"observations\"][0].dtype"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "261d573a6104c777c5bcb8f0f842b520d5ed29489b4fca410f1b3e296ea92c59"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('hwm': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
